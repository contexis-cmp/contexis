# Example: Using Hugging Face with Contexis

This example shows how to run the Contexis server and query it with Hugging Face generation enabled.

## Prerequisites

- Hugging Face access token

## Steps

1) Export environment variables:

```bash
export HF_TOKEN=...  # your token
export HF_MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct
```

2) Start the server:

```bash
ctx serve --addr :8000
```

3) Create minimal files (if not already present) for a component called `SupportBot`:

```bash
mkdir -p contexts/SupportBot prompts/SupportBot
cat > contexts/SupportBot/support_bot.ctx <<'YAML'
name: "SupportBot"
version: "1.0.0"
role:
  persona: "helpful"
YAML

cat > prompts/SupportBot/agent_response.md <<'MD'
You are a helpful assistant.
User said: {{ .data.user_input }}
Respond politely.
MD
```

4) Send a chat request:

```bash
curl -sS -X POST http://localhost:8000/api/v1/chat \
  -H 'Content-Type: application/json' \
  -d '{
    "tenant_id":"",
    "context":"SupportBot",
    "component":"SupportBot",
    "query":"",
    "top_k":0,
    "data": {"user_input": "hello"}
  }'
```

If HF env is set, the response will be generated by the configured HF model.


