# Hugging Face Integration

This guide explains how to use Hugging Face models with the Contexis runtime via the Hugging Face Inference API. The runtime will render your prompt template and, when configured, call the HF model to produce a final response.

## What you get

- Optional provider: When `HF_TOKEN` and `HF_MODEL_ID` are set, the server will call HF after prompt rendering.
- CLI utility: `ctx hf test-model "<prompt>"` to quickly validate connectivity and model output.
- Ops-ready: Prometheus metrics and OpenTelemetry tracing around inference calls.
- Kubernetes ready: Helm values and ExternalSecret wiring to pass HF env vars securely.

## Quick start

1) Set environment variables and test the model:

```bash
export HF_TOKEN=...  # your HF access token
export HF_MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct
# optional override
export HF_ENDPOINT=https://api-inference.huggingface.co/models

ctx hf test-model "Hello from Contexis!"
```

2) Run the server and send a chat request:

```bash
ctx serve --addr :8000

curl -sS -X POST http://localhost:8000/api/v1/chat \
  -H 'Content-Type: application/json' \
  -d '{
    "tenant_id":"",
    "context":"SupportBot",
    "component":"SupportBot",
    "query":"",
    "top_k":0,
    "data": {"user_input": "hello"}
  }'
```

If `HF_TOKEN` and `HF_MODEL_ID` are set, the response will be generated by the HF model from the rendered prompt.

## Configuration

Environment variables:

- `HF_TOKEN` (required): Hugging Face access token.
- `HF_MODEL_ID` (required): Model identifier, e.g. `meta-llama/Meta-Llama-3.1-8B-Instruct`.
- `HF_ENDPOINT` (optional): Inference API base URL. Defaults to `https://api-inference.huggingface.co/models`.

## Metrics and tracing

- `cmp_hf_inference_latency_seconds{model}`: Histogram of inference latency.
- `cmp_hf_inference_errors_total{code}`: Total inference errors by error code.
- OpenTelemetry span `huggingface.generate` annotated with `provider=huggingface` and `model_id`.

## Kubernetes (Helm)

Values in `charts/contexis-app/values.yaml`:

```yaml
secrets:
  HF_TOKEN: ""
  HF_MODEL_ID: ""
  HF_ENDPOINT: ""

externalSecrets:
  enabled: false
  secretStoreRef:
    kind: ClusterSecretStore
    name: vault
  data:
    - secretKey: HF_TOKEN
      remoteRef: { key: kv/data/contexis, property: HF_TOKEN }
    - secretKey: HF_MODEL_ID
      remoteRef: { key: kv/data/contexis, property: HF_MODEL_ID }
    - secretKey: HF_ENDPOINT
      remoteRef: { key: kv/data/contexis, property: HF_ENDPOINT }
```

The chart templates mount these into the app as environment variables. You can enable ExternalSecrets to source them from your secret store.

## Notes and limitations

- Initial integration uses the HF Inference API for text generation. Embeddings and local serving (e.g., TGI) can be added later.
- Prompt rendering happens before inference. Ensure your templates include all necessary context and memory.
- Keep tokens secure. Prefer Kubernetes secrets or ExternalSecrets in production.

## Troubleshooting

- 401/403 errors: Verify `HF_TOKEN` scope and validity.
- 404 errors: Check `HF_MODEL_ID` spelling and access.
- Timeouts: Increase network timeout, confirm model availability, or try a smaller model.


